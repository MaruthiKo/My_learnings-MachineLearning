{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xn_B0pB8jDm"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPSeNCZe8jDs"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0pwKx2j8jDt"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pw65rcLk8jDw",
        "outputId": "8380dc86-8a37-4ada-b172-380f624bf42c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I’ve', 'been', 'waiting', 'for', 'a', 'HuggingFace', 'course', 'my', 'whole', 'life.'], ['I', 'hate', 'this', 'so', 'much!']]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\"I’ve been waiting for a HuggingFace course my whole life.\",\n",
        "          \"I hate this so much!\"]\n",
        "tokenized_text = [sentence.split(\" \") for sentence in sentences]\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KB81vwDn8jDz"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ti1AQ9M08jD0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZWb3AgQk8jD1",
        "outputId": "d8bb4dac-0fa8-4cb9-f40b-1e4ed1c8bdb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer(sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xSnMPD5K8jD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61308714-a2c8-4ac0-c3c2-f3c638e8c1af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('directory_on_my_computer/tokenizer_config.json',\n",
              " 'directory_on_my_computer/special_tokens_map.json',\n",
              " 'directory_on_my_computer/vocab.txt',\n",
              " 'directory_on_my_computer/added_tokens.json',\n",
              " 'directory_on_my_computer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BNadAoX78jD3",
        "outputId": "6a681792-31c9-4a84-ffb2-b8919071b633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', '’', 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = sentences[0]\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WWgXj9vH8jD3",
        "outputId": "b4e79d12-8db4-4e6c-996d-b33fbe82388c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119]\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mvYdB8XF8jD4",
        "outputId": "74a0c5af-90c5-458a-9ba3-b7a9161ed909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ’ ve been waiting for a HuggingFace course my whole life.\n"
          ]
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HV8RX70lEJaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}